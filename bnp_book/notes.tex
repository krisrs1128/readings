\documentclass{article}
\usepackage{amssymb, amsfonts}
\usepackage{natbib}
\usepackage{graphicx}
\input{preamble.tex}

\title{Notes on Bayesian Nonparametrics (Cambridge Series)}
\author{Kris Sankaran}

\begin{document}
\maketitle

\section{Chapter 1}

They begin with some taxonomizing,

\begin{itemize}
\item Frequentist parametrics: $p$-values, confidence intervals, derived from
  analytical calculations and large sample approximations. Lots of optimality
  theory.
  \item Bayesian parametrics: Stems from Bayes' theorem, with finite (small)
    number of parameters. Became much more popular with the development of MCMC
    methods.
  \item Frequentist nonparametrics: Model-free (e.g., rank-based) tests, the
    bootstrap, unknown densities. All have infinite dimensional parameter spaces
    (e.g., all CDFs $F$)
  \item Bayesian nonparametrics: Place probability measure on infinite
    dimensional parameter spaces.
\end{itemize}

First two approaches are actually very similar in large samples, as made precise
by Bernstein-von Mises theorems. But caution should be exercised in bayesian
nonparametrics -- don't always have consistency or Bernstein-von Mises theorems.

We care both about the construction and the study (performance, comparisons,
etc.) of different procedures. The ideas we develop borrow from literature on
both non/semi parametrics as well as simulation. Also note that the ``two
cultures'' of statistics are both present in BNP, and create some tension.

They review some of the history, give a brief description of other books, and
describe some general research directions.

\section{Motivation and Ideas}

``It is instructive to think of all Bayesians as constructing priors on spaces
of density functions'' (e.g., normal assumption has normal shaped densities, BNP
assumptions take wider class of shapes).

A short rant against empirical bayesians.

One approach to specify nonparametric priors is to match moments of observed data to what
is expected, when sampling data from the prior, e.g.,

\begin{align}
  \mu_{1}\left(x\right) &= \int \Gsn\left(x \vert \theta, \sigma^{2}\right) \pi\left(d\theta, d\sigma\right)
\end{align}

To frame BNP ideas decision theoretically, consider quantifying the distance
between the true prior density and some parametric family $\left(f\left(x;
\theta\right)\right)$,

\begin{align}
d\left(f\left(\cdot; \theta\right), f\left(\cdot\right)\right),
\end{align}
e.g., the KL divergence. Choose $\theta$ to minimize the expected distance,
under the posterior. If you take $\Pi\left(f\right)$ to be the Bayesian
bootstrap (sample from ECDF), then the $\hat{\theta}$ that is the MLE.

A rant against Doob's theorem.

Prior serves two purpose: in addition to encoding prior beliefs, it fully
specifies the learning model (in predictive / posterior distribution terms).

Here's an idea: consider encoding the loss of choosing $\mu$ as the posterior,
using
\begin{align}
  \sum_{i} l\left(\mu, X_{i}\right) + l\left(\mu, \Pi\right)
\end{align}
for some as yet unspecified loss functions $l$. Of course the most common
version of this just takes $\mu$ to be parametric and uses $-\log f\left(X;
\theta\right)$ and $-\log\pi\left(\theta\right)$. Minimizing this gives the
posterior mode.

To be nonparametric about things, consider a KL divergence instead. It's useful
to consider losses for particular densities $f$, then average according to
$\mu$. Minimizing the resulting loss gives the full posterior.

If you mix the two optimization problems, you recover a pseudoposterior. Can be
used to ensure consistency in more cases than if you only use posterior.

\pagebreak

\section{The Dirichlet process, related priors, and posterior asymptotics}

In the nonparametric setting, you try to let the data generating process (the
parameters) be described by functions. What would we want for priors in these
nonparametric settings? THey should have large support, so that posterior
inference has good frequentist properties. E.g., consistency. Would like to
characterize rate of convergence of posterior concentration, and have it be
fast.

What would the Bayesian analog of the ECDF look like? Think of probabilities at
the observed data locations, which gives rise to a multinomial likelihood, which
leads us to a Dirichlet prior. The Dirichlet process induces this distribution
on every possible partition of the data space.

\section{Construction of the Dirichlet Process}

It's not possible to naively apply Kolmogorov's consistency theorem (the product
$\sigma$-field $\left[0, 1\right]^{\mathcal{B}}$ is not rich enough). But you
can use a countable generator instead, or try normalizing a gamma process.

There are some useful properties,

\begin{itemize}
\item For any $A$, we have the expected measure assigned to that set is
  $\Esubarg{D_{\alpha}}{P\left(A\right)} = \frac{\alpha\left(A\right)}{\alpha\left(\Omega\right)}$.
\item The corresponding variance is $\frac{G\left(A\right)G\left(A^{C}\right)}{\alpha\left(\Omega\right) + 1}$
  Note that the total mass of $\alpha$ modulates the concentration around that measure.
\item You can also compute expectations of linear functionals using the standard
  machine. Distributions can also be found, but are more complicated.
\item The DP is conjugate ``for estimating a completely unknown distribution
  from i.i.d. data.'' The posterior is $D_{\alpha + \sum \delta_{X_{i}}}$. This
  can be proven by drawing smaller partitions around the $X_{i}$ (so that you
  get these delta point masses at the observations) and using a martingale
  convergence theorem.
\item The posterior mean is a mixture of prior mean and the empirical CDF.
\item In the limit, you recover either the ECDF, brownian bridge, or bayesian
  bootstrap, depending on the type of asymptotic.
\item The DP isn't smooth (just because there's a point \textit{near} $A$
  doesn't mean the posterior mean of $A$ will go up. In fact, due to negative
  correlation across partitions, it will go down. Also, for all the promise of
  being nonparametric, the process is always discrete (though, consider an
  analogy with the ECDF).
\item If you analyze / condition on subsets, you recover a DP which is
  independent of the complement. This is called self-similarity, and is the
  motivation behind tail-free processes.
\item Assuming some properties in the limiting concentration parameter, you have
  consistency regardless of the choice of prior, which is odd in the Bayesian
  context.
\item You have the predictive,
  \begin{align*}
    X_{n} \vert X_{1:(n - 1)} &\sim \frac{M}{M + n - 1}G + \sum \frac{n_{j}}{M + n - 1} \delta_{\theta_{j}}
  \end{align*}
\item There's the stick breaking representation,
\begin{align*}
  \sum_{i = 1}^{\infty} V_{i}\delta_{\theta_{i}}
\end{align*}
where
\begin{align*}
  V_{i} &= Y_{i}\prod_{j = 1}^{i - 1}\left(1 - Y_{j}\right)
\end{align*}
and the $Y_{j}$ are iid $\Bet\left(1, M\right)$. The proof uses the
self-similarity property.
\item The prior and posterior are mutually singular. Of course there is no
  dominating measure.
\item The tails are lighter than the base measure $G$.
\end{itemize}

\section{Priors related to the DP}

I wasn't taking notes on this, but they talked about DP mixtures (mix over the
concentration parameter, e.g.), the DPMM, and the HDP.

They've also spent a bit time discussing posterior consistency (failure
situations too). They outlined Schwartz's argument and the KL property which
guarantees consistency.

\section{General Theory (consistency)}

Need numerator (likelihood times prior) to be upper bounded by
$\exp{-cn\eps_{n}^{2}}$ and the denominator (marginal likelihood) to be lower
bounded by $\exp{-bn\eps_{n}^{2}}$.

For the denominator, can replace the KL positivity condition. Still need a prior
concentration rate,
\begin{align}
\Pi\left(B\left(p_{0}, \eps_{n}\right)\right) \geq \exp{-b_{1}n\eps_{n}^{2}},
\end{align}
where the exponent goes to $\infty$ since density estimation problems must have
rate slower than $n^{-\frac{1}{2}}$.

For the numerator, use seives + metric entropy.

\section{Applications}

\begin{itemize}
\item Can use bracketing to get rates.
\item The $\eps$-bracketing number of a class of densities is the number of
  pairs (which are within $\eps$ of each other) of densities that are needed
  so that any individual density is in at least one of these brackets.
\item Can be used to show that get minimax rate $n^{-\frac{\alpha}{2\alpha +
    1}}$ in Holder continuous densities (smoothness $\alpha$)
\item Can get results for parametric models and log spline priors, but need
  a more refined notation of entropy (local entropy) to get correct rates
  (otherwise have extra log factor)
\item This general theory can show that the posterior converges at the minimax
  rate in Dirichlet process mixture models and Gaussian processes.
\end{itemize}

In the misspecified case (true model does not lie in the posited model space),
you converge to the KL projection of the truth onto the model space.

There are also extensions to the non i.i.d. case (mainly independendent
non-identical and Markov processes, apparently).

\section{Adaptation and model selection}

In classical theory, you often see oracle properties, where an estimator gets a
rate equal to what you would have gotten if you actually knew the smoothness
level of the target density. In the bayesian case, you would hope to get similar
kinds of adaptiveness by placing a prior on the smoothness parameter, so we have
a two-level hierarchical model.

This can be formalized in a general theory, which says that if you have a local
entropy condition and a few bounds on the relative probability of sets under too
coarse / too smooth priors, then this addaptation strategy is effective.

\section{Bernstein-von Mises theorems}

On top of knowing that the posterior converges at some rate, we would like to
know the actual shape of the posterior. Bernstein-von Mises says that the
posterior is asymptotically normal, centered around the MLE with inverse
Fisher-information covariance. So, frequentist and Bayesian inference coincide.

There a few nonparametric versions of these theorems. E.g., the Dirichlet
Process analog of Donsker is that $\sqrt{n}\left(F - \mathbb{F}_{n}\right)$
converges to an $F_{0}$ Brownian Bridge under $F_{0}$.

There are cases where these theorems don't obtain. See Cox 1993 or Freedman
1999.

\section{Chapter 3}
\label{sec:chapter_3}

We'll start on mixture modeling (haven't been typing up my notes so much
lately). The setup is the usual DPMM, draw the $\theta_{i}$'s from a Dirichlet
Process, and then have continuous observations $X_{i}$ drawn from
$F_{\theta_{i}}$. Since many of the $\theta_{i}$s are equal to one another, this
induces a mixture model on the $X_{i}$.

The posterior for the $\theta_{i}^{\ast}$s looks like
\begin{align}
\Pi_{k}^{(n)}\left(n_{1}, \dots, n_{i}\right) \prod_{j} \prod_{i \in C_{j}} k\left(x_{i}, \theta_{j}^{\ast}\right) P_{0}\left(d\theta_{j}^{\ast}\right).
\end{align}
Here the $\Pi_{k}^{(n)}$ gives the prior for the observed partition structure,
the $P_{0}\left(d\theta_{j}^{\ast}\right)$ gives the prior for the observed
locations of the mixture components, the $k$ gives a likelihood of the $x_{i}$s
given their component.

From here, you can get a bayesian posterior density estimate for the
hierarchical mixture model, but it requries a sum over all partitions of $n$
into $k$ components (for all $k \leq n$). So you need to do MCMC, like in
Escobar and West. While most people write the algorithm for the DPMM, all you
really need is the predictive of a new $\theta_{j}^{\ast}$ given the observed
ones, along with a way to evaluate the EPPFs $\Pi_{k}^{(n)}$.

The idea of the gibbs sampling scheme is to draw each $\theta_{i}^{(t)}$ from
the full conditional on all other $\theta_{i}^{(t-1) or t}$s, using the
predictive distributions, which can be written in terms of the EPPFs
$\Pi_{k_{i, n - 1} + 1}^{(n)}$ and $\Pi_{k_{i}, n -1}^{(n)}$. An issue is that
clusters will switch very slowly, since you have to pass through valleys in the
posterior corresponding to half-swapped clusters. One remedy is to model the
posterior directly -- these are called conditional methods, as opposed to the
``marginal'' methods presented above.

In the Poisson Dirichlet case, the update weights are
\begin{align}
\left(\theta + \sigma k_{i, n - 1}\right) \int_{\Theta}k\left(x_{i}, \theta\right)P_{0}\left(d\theta\right)
\end{align}
for a new cluster component and
\begin{align}
\left(n_{i, j} - \sigma\right)k\left(x_{i}, \theta_{ij}^{\ast}\right)
\end{align}
for a previously observed cluster.

One interesting idea is to put a prior on $\sigma$ in the PD model -- you can
get an updated posterior, which helps you learn the relative sizes / total
numbers of clusters.

Another example is random bernstein polynomials. The idea is that this family
uniformly approximates any $C\left[0, 1\right]$ function. So you can construct
random members of this family by randomizing the degree and mixture weights --
these look like mixtures of different $\beta$ densities. There is also a latent
variable interpretation of this model, which leads to a suitable gibbs sampler
-- see Petrone 1999.

Now moving on to polya trees. Let $\Gamma = \left(\Gamma_{k}\right)$ be a family
of nested partitions (get refinements with increasing $k$). A density
$\tilde{p}$ is called tail-free if there exists some R.V.s $V_{k, B}$ such that

\begin{itemize}
\item $\{V_{1, B}: B\in \Gamma_{1}\}, \{V_{2, B} : B \in \Gamma_{2}\}, \dots, $ are independent
\item If $B_{k} \subset B_{k - 1} \subset \dots \subset B_{1}$, then
  $\tilde{p}\left(B_{k}\right) = \prod_{j = 1}^{k} V_{j, B_{j}}$.
\end{itemize}

The special case where the $\Gamma_{k}$s are dyadic intervals and the
$V_{j, B_{j}}$ are beta R.V.s gives the Polya tree process. It turns out that
the posterior of the polya tree is still a polya tree. You can write it down
pretty directly as the analogous sequences of beta distributed variables where
the parameters are updated by counts of the number of points within the
associated partition bin.

It's possible to get priors that put more mass on different parts of the
interval by sizing the bins unevenly. While it's neat that polya trees put
priors on continuous densities, a disadvantage is that they can be dependent on
the specific partition structure.

Now random means. This means that we are studying
$\tilde{p}\left(f\right) = \int f d\tilde{p}$ for some function $f$.
An interesting identity when $\tilde{p}$ is a dirichlet process is the
regazzini-cifarelli identity:

\begin{align}
  \Earg{\frac{1}{\left(1 + i t \tilde{p}\left(f\right)\right)^{\theta}}} &= \exp{-\int \log\left(1 + itf\right) d\alpha},
\end{align}
the interpretation is the left hand side is the stieltjes transform of
$\tilde{p}\left(f\right)$ while the right hand side is a laplace transform of
$\int f d\tilde{\gamma}$ under a gamma process prior $\tilde{gamma}$.

Hook walks? Young diagram? What is this black magic??

Closing remarks: It's valuable to think about frequentist properties of bayesian
models. De Finetti is the ultimate motivating theorem for this type of analysis.

\section{Chapter 6}
\label{sec:chapter_6}

What computational issues arise when working with bayesian nonparametric
objects (considering things are infinite dimensional, shouldn't be surprising
that there are a few)?

``we see no reason why in general Nature should adhere to a small collection of
computationally convenient model forms.''

Basic motivation for bayesian modeling comes from de Finetti representation,

\begin{align}
p\left(x_{1}, \dots, x_{n}\right) &= \int_{T} \prod_{i = 1}^{n} F\left(x_{i}\right) dQ\left(F\right).
\end{align}
You can think of $Q\left(F\right) = \lim_{n} p\left(F_{n}\right)$ where $F_{n}$
are empirical CDFs. In parametric modeling, we assume restricted form of $Q$ --
idea in bnp is to remove this.

For computation in the DPMM, it is useful to consider a latent variable
interpretation, where the latent variables are cluster memberships and the
probabilities for the different clusters come from a stick breaking / GEM.


Alternatively, you can integrate out the cluster positions and work only with
the partition structure, via a polya urn scheme. This can be done in closed form
when the likelihood is conjugate to the prior for a single component, since

\begin{align}
p\left(s_{1}, \dots, s_{n}\vert y\right) \propto p\left(s_{1}, \dots, s_{n}\right) p\left(y \vert s_{1}, \dots, s_{n}\right),
\end{align}
and the prior term can be computed from a polya urn and the likelihood is just a
mixture once the components are known. This can be used to derive a gibbs
sampling scheme.

Mixing can be slow since cluster memberships have to be updated one at a time,
and there might be low probability valleys between two modes. An alternative is
the split-merge (or ebb-flow) algorithm.

Sometimes we don't have to have to rely on integrating out the nonparametric
$F$. Some alternatives are truncation, retrospective sampling, and slice
sampling.

There sampling in the truncation scheme is actually approximate. But you can try
to bound the TV distance between the truncated and full priors, as a function of
$K$, and use this to set $K$. Then, the stick breaking weights and model
parameters can be updated in blocks.

The idea in retrosupective sampling is to use inversion sampling for discrete
distributions. Draw a uniform and find the $K$ such that the sum of stick
breaking weights up to $K$ is larger than that uniform. This $K$ is random and
supported on all natural numbers, but in any given iteration it will be finite.
You can then make an update for all parameters via mcmc.

In slice sampling, you simulate uniforms $u_{1}, \dots, u_{n}$ such that the
joint $p\left(s_{i} = k, u_{i}\right) = \indic{u_{i} < w_{k}}$, where the
$s_{i}$ are cluster membership variables. You can then setup a gibbs sampler,
since the posterior is proportional to
\begin{align}
p\left(w_{1}, \dots, w_{K}\right)\prod_{1}^{n} \indic{u_{j} < w_{s_{j}}} g\left(y_{j} \vert \theta_{s_{j}}\right) \prod_{1}^{K} h\left(\theta_{i}\right)
\end{align}
where $K$ is chosen so the amount of stick left in the stick breaking process is
less than $\min\left(u_{i}\right)$.

Slice sampling can also be extended to normalized CRMs, using a clever
application of the Levy-Khintchine formula to take an infinite sum into a
one-dimensional integral.

\section{Chapter 7}
\label{sec:chapter_7}

It's common in biomedicine these days to make high-dimensional measurements.
Classical nonparametric methods can fail in this setting, some sort of sharing
or latent variable reductions are necessary, and this can be accomplished in the
BNP paradigm. Another advantage is that complex study structure can often be
modeled easily probabilistically.

A motivating example is an extension of the usual random effects model for the
$j^{th}$ observation in subject $i$ to
\begin{align}
  \label{eq:simple_re}
  y_{ij} &= \Gsn\left(\mu_{i}, \sigma^{2}\right) \\
  \mu_{i} &\sim P,
\end{align}
for some nonparametric $P$ instead of the usual
$\mu_{i} \sim \Gsn\left(\mu, \tau^{2}\right)$. You might imagine there are
latent subpopulations, for example, which would warrant several modes in $P$.

Apparently in a DP with $\alpha = 10$, we have $\infty \approx 50$ (sigh...).

``It is important not to view the DP model as a magic clustering approach, which
avoids assuming a fixed number of clusters and specification of an arbitrarily
penalty. Instead, one must carefully consider how the penalty for model
complexity or overfitting arises in the DP implementation....''

A problem that people don't often talk about is sensitivity of the DP
specification to the choice of base measure (as opposed to concentration, which
everyone knows is related to the fitted $K$). The issue is that in the polya
scheme, the probability of choosing a new cluster is proportional to

\begin{align}
\int \prod_{j = 1}^{n} \Gsn\left(y_{ij}; \mu_{i}, \sigma^{2}\right) dP_{0}\left(\mu_{i}\right)
\end{align}
which can depend a lot on (say) the mean and variance of $P_{0}$ if it is
normal. E.g., if $\sigma^{2}$ is too small, we'll introduce new clusters all the
time, while if it's too big, we won't ever introduce new clusters. The usual
solution is to make an informed choice by looking at the data, or to just
standardize everything.

On to posterior computation. The three main types of MCMC people use are
collapsed gibbs, block gibbs, and reversible-jump type samplers. In the
collapsed samplers, the stick breaking weights are integrated out and the
partition structure is sampled instead, via a polya urn. In the block gibbs
samplers, a truncation is applied to the original stick breaking representation
instead. The updates look like,

\begin{itemize}
\item Allocate individuals to components
\item Update stick breaking weights
\item Update the atoms (model parameters)
\item Update hyperparameters, and hyperhyperparameters
\end{itemize}

When people ask about the difference between a parametric model with some $K$
and a truncated DPMM, you can say that the $K$ in the DPMM is more like an upper
bound for the number of clusters.

We can extend the random effects model \ref{eq:simple_re} to

\begin{align}
  y_{ij} &\sim \Gsn\left(x_{ij}^{T}\beta + z_{ij}^{T}b_{i}, \sigma^{2}\right)\\
  b_{i} &\sim P
\end{align}
for random effects $b_{i}$. An issue with interpretation of the fixed effects
$\beta$ is that we don't necessarily have to have $P$ have zero mean. There are
some proposals in the literature for addressing this though.

Often, instead of random effects, we want latent variables instead. This can
look like

\begin{align}
  y_{ij} &\sim \Gsn\left(\mu_{j} + \lambda_{j} \eta_{i}, \sigma^{2}\right) \\
  \eta_{i} &= x_{i}^{T}\beta + \delta_{i} \\
  \delta_{i} &\sim P
\end{align}
where you interpret $\eta_{i}$ as scores and $\lambda_{j}$ as loadings.

Now on to functional data analysis.

\end{document}
